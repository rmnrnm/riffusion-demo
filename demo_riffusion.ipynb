{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmnrnm/riffusion-demo/blob/main/demo_riffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# riffusion colab demo\n",
        "\n",
        "Run [riffusion](https://www.riffusion.com/about) in a gradio demo with a colab host\n",
        "\n",
        "Riffusion project by [Seth Forsgren](https://twitter.com/sethforsgren) and [Hayk Martiros](https://github.com/hmartiro), colab notebook by [Jasper Gilley](https://twitter.com/0xjasper)\n",
        "\n",
        "Feel free to DM Jasper on Twitter if you have any problems with the notebook\n",
        "\n",
        "Some cool prompt ideas can be found at https://ai-art-wiki.com/wiki/Riffusion#Prompts"
      ],
      "metadata": {
        "id": "HK0XvGHcKeos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wtPb1LmTup-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhX8wJzIugkh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Clone the inference repo\n",
        "!git clone https://github.com/riffusion/riffusion.git\n",
        "%cd riffusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -e .\n",
        "!pip install Pillow==9.0.0"
      ],
      "metadata": {
        "id": "vEpm84ugPuKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install requirements (you may need to restart the kernel after this)\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "wNrYejGYuyws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import io\n",
        "import numpy as np\n",
        "import dataclasses\n",
        "import IPython.display as ipd\n",
        "from PIL import Image\n",
        "import pydub\n",
        "from random import randint\n",
        "\n",
        "import PIL.Image    \n",
        "if not hasattr(PIL.Image, 'Transpose'):  \n",
        "    PIL.Image.Transpose = PIL.Image  \n",
        "\n",
        "from riffusion.streamlit import util\n",
        "from riffusion.spectrogram_params import SpectrogramParams\n",
        "from riffusion.datatypes import InferenceInput, PromptInput\n"
      ],
      "metadata": {
        "id": "FwUldklBwbEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = util.load_riffusion_checkpoint(\n",
        "    device=\"cuda\",\n",
        "    checkpoint=util.DEFAULT_CHECKPOINT,\n",
        "    # No trace so we can have variable width\n",
        "    no_traced_unet=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "cJEn1fFHsZN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_audio_from_spectrogram(image):\n",
        "    params = SpectrogramParams(\n",
        "        min_frequency=0,\n",
        "        max_frequency=10000,\n",
        "    )\n",
        "    segment = util.audio_segment_from_spectrogram_image(\n",
        "        image=image,\n",
        "        params=params,\n",
        "        device=\"cuda\",\n",
        "    )\n",
        "    return segment\n",
        "\n",
        "def create_spectrogram_from_prompt(prompt, negative_prompt, guidance=7, seed=42, width=512):\n",
        "    image = util.run_txt2img(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=30,\n",
        "        guidance=7,\n",
        "        negative_prompt=negative_prompt,\n",
        "        seed=seed,\n",
        "        width=width,\n",
        "        height=512,\n",
        "        checkpoint=\"riffusion/riffusion-model-v1\",\n",
        "        device=\"cuda\",\n",
        "        #scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    ipd.display(image)\n",
        "    \n",
        "    return image\n",
        "\n",
        "\n",
        "  \n",
        "def render(prompt_a, prompt_b, num_interpolation_steps=5, seed_a=None, seeb_b=None, negative_prompt_a=False, negative_prompt_b=False, denoising=.75):\n",
        "    \n",
        "    \"\"\"\n",
        "    Interpolate between prompts in the latent space.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    This tool allows specifying two endpoints and generating a long-form interpolation\n",
        "    between them that traverses the latent space. The interpolation is generated by\n",
        "    the method described at https://www.riffusion.com/about. A seed image is used to\n",
        "    set the beat and tempo of the generated audio, and can be set in the sidebar.\n",
        "    Usually the seed is changed or the prompt, but not both at once. You can browse\n",
        "    infinite variations of the same prompt by changing the seed.\n",
        "    For example, try going from \"church bells\" to \"jazz\" with 10 steps and 0.75 denoising.\n",
        "    This will generate a 50 second clip at 5 seconds per step. Then play with the seeds\n",
        "    or denoising to get different variations.\n",
        "    \"\"\"\n",
        "\n",
        "    #device = \"cuda\"\n",
        "    extension = \"mp3\"\n",
        "\n",
        "    num_inference_steps = 30\n",
        "\n",
        "    guidance = 7 # How much the model listens to the text prompt\n",
        "\n",
        "    init_image_name = \"og_beat\"\n",
        "    \n",
        "\n",
        "    alpha_power = 1\n",
        "\n",
        "\n",
        "    alphas = np.linspace(0, 1, num_interpolation_steps)\n",
        "\n",
        "    # Apply power scaling to alphas to customize the interpolation curve\n",
        "    alphas_shifted = alphas * 2 - 1\n",
        "    alphas_shifted = (np.abs(alphas_shifted) ** alpha_power * np.sign(alphas_shifted) + 1) / 2\n",
        "    alphas = alphas_shifted\n",
        "\n",
        "    if seed_a is None:\n",
        "        seed_a = randint(1,9999)\n",
        "        \n",
        "    if seeb_b is None:\n",
        "        seeb_b = randint(1,9999)\n",
        "\n",
        "    # Prompt inputs A and B in two columns\n",
        "\n",
        "    prompt_input_a = PromptInput(\n",
        "        guidance=guidance, **get_prompt_inputs(prompt_a, negative_prompt_a, seed=seed_a, denoising_default=denoising)\n",
        "    )\n",
        "\n",
        "    prompt_input_b = PromptInput(\n",
        "        guidance=guidance, **get_prompt_inputs(prompt_b, negative_prompt_b, seed=seeb_b, denoising_default=denoising)\n",
        "    )\n",
        "\n",
        "\n",
        "    init_image_path = os.path.join(\"seed_images\", f\"{init_image_name}.png\")\n",
        "    init_image = Image.open(str(init_image_path)).convert(\"RGB\")\n",
        "\n",
        "    # TODO(hayk): Move this code into a shared place and add to riffusion.cli\n",
        "    image_list: T.List[Image.Image] = []\n",
        "    audio_bytes_list: T.List[io.BytesIO] = []\n",
        "    for i, alpha in enumerate(alphas):\n",
        "        inputs = InferenceInput(\n",
        "            alpha=float(alpha),\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            seed_image_id=\"og_beat\",\n",
        "            start=prompt_input_a,\n",
        "            end=prompt_input_b,\n",
        "        )\n",
        "\n",
        "        if i == 0:\n",
        "            print(\"Example input JSON\")\n",
        "            print(dataclasses.asdict(inputs))\n",
        "\n",
        "        image, audio_bytes = run_interpolation(\n",
        "            pipeline,\n",
        "            inputs=inputs,\n",
        "            init_image=init_image,\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        image_list.append(image)\n",
        "        audio_bytes_list.append(audio_bytes)\n",
        "\n",
        "    # TODO(hayk): Concatenate with overlap and better blending like in audio to audio\n",
        "    audio_segments = [pydub.AudioSegment.from_file(audio_bytes) for audio_bytes in audio_bytes_list]\n",
        "    concat_segment = audio_segments[0]\n",
        "    for segment in audio_segments[1:]:\n",
        "        concat_segment = concat_segment.append(segment, crossfade=0)\n",
        "\n",
        "    audio_bytes = io.BytesIO()\n",
        "    concat_segment.export(audio_bytes, format=extension)\n",
        "    audio_bytes.seek(0)\n",
        "\n",
        "    print(f\"Duration: {concat_segment.duration_seconds:.3f} seconds\")\n",
        "    #ipd.display(ipd.Audio(audio_bytes))\n",
        "\n",
        "    output_name = (\n",
        "        f\"{prompt_input_a.prompt.replace(' ', '_')}_\"\n",
        "        f\"{prompt_input_b.prompt.replace(' ', '_')}.{extension}\"\n",
        "    )\n",
        "    return audio_bytes, concat_segment\n",
        "    \n",
        "\n",
        "def get_prompt_inputs(\n",
        "    prompt,\n",
        "    negative_prompt,\n",
        "    seed,\n",
        "    denoising_default: float = 0.5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute prompt inputs from widgets.\n",
        "    \"\"\"\n",
        "    p = {}\n",
        "\n",
        "    p[\"prompt\"] = prompt\n",
        "\n",
        "    if negative_prompt:\n",
        "        p[\"negative_prompt\"] = negative_prompt\n",
        "\n",
        "    p[\"seed\"] = seed\n",
        "\n",
        "    p[\"denoising\"] = denoising_default\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def run_interpolation(\n",
        "    pipeline,\n",
        "    inputs: InferenceInput,\n",
        "    init_image: Image.Image,\n",
        "    extension: str = \"mp3\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Cached function for riffusion interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    image = pipeline.riffuse(\n",
        "        inputs,\n",
        "        init_image=init_image,\n",
        "        mask_image=None,\n",
        "    )\n",
        "\n",
        "    # TODO(hayk): Change the frequency range to [20, 20k] once the model is retrained\n",
        "    params = SpectrogramParams(\n",
        "        min_frequency=0,\n",
        "        max_frequency=10000,\n",
        "    )\n",
        "\n",
        "    # Reconstruct from image to audio\n",
        "    audio_bytes = util.audio_bytes_from_spectrogram_image(\n",
        "        image=image,\n",
        "        params=params,\n",
        "        device=\"cuda\",\n",
        "        output_format=extension,\n",
        "    )\n",
        "\n",
        "    return image, audio_bytes"
      ],
      "metadata": {
        "id": "nwHJlDcaPdSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run with Colab interface\n",
        "\n",
        "prompt= 'dog barking'#@param {type:\"string\"}\n",
        "negative_prompt = \"\"#@param {type:\"string\"}\n",
        "\n",
        "\n",
        "image = create_spectrogram_from_prompt(prompt, \n",
        "                                       negative_prompt, \n",
        "                                       guidance=20,\n",
        "                                       seed=2)\n",
        "\n",
        "make_audio_from_spectrogram(image)"
      ],
      "metadata": {
        "id": "uO7oM1UGsL5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_bytes, concat_segment = render('piano', 'oboe', num_interpolation_steps=5)"
      ],
      "metadata": {
        "id": "hgiOk_MORq4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat_segment"
      ],
      "metadata": {
        "id": "TOu98mjoRwXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Audio to Audio\n"
      ],
      "metadata": {
        "id": "TYttS1elXztX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "extension = \"mp3\"\n",
        "checkpoint =util.DEFAULT_CHECKPOINT,\n",
        "\n",
        "use_20k = False\n",
        "use_magic_mix = False\n",
        "\n",
        "num_inference_steps=30\n",
        "guidance=7\n",
        "scheduler=util.SCHEDULER_OPTIONS[0]\n"
      ],
      "metadata": {
        "id": "42elsn75R6Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment = util.load_audio_file('/content/0QMxekxU7CEDP04eSmDi1b.mp3')"
      ],
      "metadata": {
        "id": "x311CdkQR-g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment"
      ],
      "metadata": {
        "id": "nklpmae2Y-6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if segment.frame_rate != 44100:\n",
        "    st.warning(\"Audio must be 44100Hz. Converting\")\n",
        "    segment = segment.set_frame_rate(44100)\n",
        "print(f\"Duration: {segment.duration_seconds:.2f}s, Sample Rate: {segment.frame_rate}Hz\")"
      ],
      "metadata": {
        "id": "1ZkmfDW5ZAPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time_s = 0\n",
        "clip_duration_s = 10\n",
        "overlap_duration_s = .1\n",
        "\n",
        "duration_s = min(clip_duration_s, segment.duration_seconds - start_time_s)\n",
        "increment_s = clip_duration_s - overlap_duration_s\n",
        "clip_start_times = start_time_s + np.arange(0, duration_s - clip_duration_s, increment_s)"
      ],
      "metadata": {
        "id": "G1mDru6qZbLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_start_times = np.arange(0,20,5)\n",
        "clip_start_times"
      ],
      "metadata": {
        "id": "aMtKnOFRc-sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(0, duration_s - clip_duration_s, increment_s)"
      ],
      "metadata": {
        "id": "btAJqIMCaTtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate=False\n",
        "denoising_default = 0.55\n",
        "seed_a = randint(1,9999)\n",
        "seed_b = randint(1,9999)\n",
        "\n"
      ],
      "metadata": {
        "id": "CNHcWCZ_aUS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate=True\n",
        "prompt_a = \"piano\"\n",
        "prompt_b = \"oboe\"\n",
        "negative_prompt_a = \"\"\n",
        "negative_prompt_b = \"\""
      ],
      "metadata": {
        "id": "0k43d9gDe8SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if interpolate:\n",
        "\n",
        "    prompt_input_a = PromptInput(\n",
        "        guidance=guidance, **get_prompt_inputs(prompt_a, negative_prompt_a, seed=seed_a, denoising_default=denoising_default)\n",
        "    )\n",
        "\n",
        "    prompt_input_b = PromptInput(\n",
        "        guidance=guidance, **get_prompt_inputs(prompt_b, negative_prompt_b, seed=seed_b, denoising_default=denoising_default)\n",
        "    )\n",
        "\n",
        "elif use_magic_mix:\n",
        "    prompt_input_a = PromptInput(\n",
        "                  prompt=prompt_a,\n",
        "                  seed=seed_a,\n",
        "                  guidance=guidance,\n",
        "              )\n",
        "    magic_mix_kmin = 0.3\n",
        "    magic_mix_kmax = 0.5\n",
        "    magic_mix_mix_factor = 0.5\n",
        "  \n",
        "else:\n",
        "    prompt_input_a = PromptInput(\n",
        "        guidance=guidance, **get_prompt_inputs(prompt_a, negative_prompt_a, seed=seed_a, denoising_default=denoising_default)\n",
        "    )"
      ],
      "metadata": {
        "id": "pyAZdhbGbSt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slice_audio_into_clips(\n",
        "    segment: pydub.AudioSegment, clip_start_times, clip_duration_s\n",
        "):\n",
        "    \"\"\"\n",
        "    Slice an audio segment into a list of clips of a given duration at the given start times.\n",
        "    \"\"\"\n",
        "    clip_segments = []\n",
        "    for i, clip_start_time_s in enumerate(clip_start_times):\n",
        "        clip_start_time_ms = int(clip_start_time_s * 1000)\n",
        "        clip_duration_ms = int(clip_duration_s * 1000)\n",
        "        clip_segment = segment[clip_start_time_ms : clip_start_time_ms + clip_duration_ms]\n",
        "\n",
        "        # TODO(hayk): I don't think this is working properly\n",
        "        if i == len(clip_start_times) - 1:\n",
        "            silence_ms = clip_duration_ms - int(clip_segment.duration_seconds * 1000)\n",
        "            if silence_ms > 0:\n",
        "                clip_segment = clip_segment.append(pydub.AudioSegment.silent(duration=silence_ms))\n",
        "\n",
        "        clip_segments.append(clip_segment)\n",
        "\n",
        "    return clip_segments\n",
        "\n",
        "\n",
        "clip_segments = slice_audio_into_clips(\n",
        "        segment=segment,\n",
        "        clip_start_times=clip_start_times,\n",
        "        clip_duration_s=clip_duration_s,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "t6QFsZHMcWx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for segment in clip_segments:\n",
        "  segment"
      ],
      "metadata": {
        "id": "zCPU4HoMcigc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment"
      ],
      "metadata": {
        "id": "VKfRKeidcxxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from riffusion import spectrogram_image_converter \n",
        "\n",
        "def spectrogram_image_from_audio(\n",
        "    segment: pydub.AudioSegment,\n",
        "    params: SpectrogramParams,\n",
        "    device: str = \"cuda\",\n",
        ") -> Image.Image:\n",
        "    converter = spectrogram_image_converter(params=params, device=device)\n",
        "    return converter.spectrogram_image_from_audio(segment)"
      ],
      "metadata": {
        "id": "J26TIEoRc0rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if interpolate:\n",
        "    # TODO(hayk): Make not linspace\n",
        "    alphas = list(np.linspace(0, 1, len(clip_segments)))\n",
        "\n",
        "for i, clip_segment in enumerate(clip_segments):\n",
        "    print(f\"### Clip {i} at {clip_start_times[i]:.2f}s\")\n",
        "\n",
        "    audio_bytes = io.BytesIO()\n",
        "    clip_segment.export(audio_bytes, format=\"wav\")\n",
        "\n",
        "    init_image = spectrogram_image_from_audio(\n",
        "        clip_segment,\n",
        "        params=params,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # TODO(hayk): Roll this into spectrogram_image_from_audio?\n",
        "    init_image_resized = scale_image_to_32_stride(init_image)\n",
        "\n",
        "    progress_callback = None\n",
        "\n",
        "\n",
        "    if interpolate:\n",
        "        assert use_magic_mix is False, \"Cannot use magic mix and interpolate together\"\n",
        "        inputs = InferenceInput(\n",
        "            alpha=float(alphas[i]),\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            seed_image_id=\"og_beat\",\n",
        "            start=prompt_input_a,\n",
        "            end=prompt_input_b,\n",
        "        )\n",
        "\n",
        "        image, audio_bytes = run_interpolation(\n",
        "            inputs=inputs,\n",
        "            init_image=init_image_resized,\n",
        "            device=device,\n",
        "            checkpoint=checkpoint,\n",
        "        )\n",
        "    elif use_magic_mix:\n",
        "        assert not prompt_input_a.negative_prompt, \"No negative prompt with magic mix\"\n",
        "        image = util.run_img2img_magic_mix(\n",
        "            prompt=prompt_input_a.prompt,\n",
        "            init_image=init_image_resized,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance,\n",
        "            seed=prompt_input_a.seed,\n",
        "            kmin=magic_mix_kmin,\n",
        "            kmax=magic_mix_kmax,\n",
        "            mix_factor=magic_mix_mix_factor,\n",
        "            device=device,\n",
        "            scheduler=scheduler,\n",
        "            checkpoint=checkpoint,\n",
        "        )\n",
        "    else:\n",
        "        image = util.run_img2img(\n",
        "            prompt=prompt_input_a.prompt,\n",
        "            init_image=init_image_resized,\n",
        "            denoising_strength=prompt_input_a.denoising,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance,\n",
        "            negative_prompt=prompt_input_a.negative_prompt,\n",
        "            seed=prompt_input_a.seed,\n",
        "            progress_callback=progress_callback,\n",
        "            device=device,\n",
        "            scheduler=scheduler,\n",
        "            checkpoint=checkpoint,\n",
        "        )\n",
        "\n",
        "    # Resize back to original size\n",
        "    image = image.resize(init_image.size, Image.BICUBIC)\n",
        "\n",
        "    result_images.append(image)"
      ],
      "metadata": {
        "id": "C_p3ubDjferg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bv9qNuuIf95G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        params = SpectrogramParams(\n",
        "            min_frequency=0,\n",
        "            max_frequency=10000,\n",
        "            stereo=False,\n",
        "        )"
      ],
      "metadata": {
        "id": "Ls0ICQ-KgBIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}